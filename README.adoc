
= Automated ModelOps Benchmarking
// Model selection is not an easy task. How can you find the best model for your use case? Is the model fast enough and is it accurate enough? Even if you narrow down the choice of models to a select few you will need to test and evaluate them on your custom data that you're expecting for your use case. Many organizations restrict access to GPUs and to sites to download models. 


// There is a growing need inside organizations to make a variety of LLMs available to application teams. Whether that is achieved through Models as a Service (MaaS) or through dedicated environments, there needs to be a standard process in place to bring those models in. Opening the network to allow everybody download models is not recommended, so there should be a team responsible for implementing the process. The team responsible for this should be the IT Operations team, or as we'll call them throughout this tutorial, the ModelOps team. Simply bringing a model in and deploying it is not enough. ModelOps teams should run performance benchmarks on the model to understand and publish how the model performs on the organization's infrastructure. Accuracy tests against various evaluation datasets should also be executed and published internally. These performance and accuracy benchmarks allow application teams to narrow down the LLMs that best fit the requirements of their use case (e.g. chatbot or batch process).

// Application developers or, as we'll refer to them in the rest of this tutorial, AI Engineers, need to be able to find LLMs that best fit their use cases. While the default benchmarks and evaluations are a great starting point in finding the best LLM for the job, they don't give you results based on a use case. To know how an LLM performs for a use case you need to run benchmarks with data that is representative of the use case. These custom prompt and accuracy evaluation datasets show you how well or how poorly an LLM performs in benchmark testing.

*The LLM Selection Problem*

As organizations rush to adopt AI they quickly find that not all LLMs are created equal and model selection is not easy. Allowing developers to download and deploy any model is unsustainable. It creates massive security vulnerabilities, leads to resource waste, and provides no clear way to compare model capabilities for a given business problem.

*The Solution: A Two-Part ModelOps Strategy*

A successful enterprise AI strategy requires a formal process for managing the LLM lifecycle. This involves two key groups: a central Operations or ModelOps team and the AI Engineers building applications.

*Part 1: The ModelOps Team*

The ModelOps team acts as the gatekeeper. They are responsible for:

* Secure Ingestion: Creating an OpenShift pipeline to safely bring new models inside the organization's network.

* Baseline Validation: Before a model is made available for internal use, baseline benchmarks with GuideLLM and lm-eval harness should be run and published.

** Infrastructure Performance (GuideLLM): How fast does this model run on our hardware?

** General Accuracy (lm-eval): How does it score on standard academic evaluation datasets?

This process creates a central registry (OpenShift AI model registry) of approved models that gives teams a safe, documented starting point.

image::images/modelop_diagram.png["ModelOps pipeline"]

*Part 2: The AI Engineer*

AI Engineers can now browse the internal registry and use the baseline benchmarks to create a shortlist. However, a general-purpose benchmark doesn't guarantee performance for a specific use case.

Therefore, the AI Engineer's workflow becomes:

* Discovery: Find a promising model in the OpenShift AI model registry based on the ModelOps team's general benchmarks.

* Specific Evaluation: Run that model through an OpenShift AI pipeline using their own custom, use-case-specific data.

This final step is what provides the definitive answer, allowing the AI Engineer to confidently determine if a model's performance and accuracy are a true fit for their unique application.

image::images/ai_engineer_diagram.png["AI Engineer Pipeline"]

== Learning Outcomes
- Implement a Two-Part ModelOps Strategy: Clearly distinguish between the responsibilities of the ModelOps or IT Operations team and a consuming AI Engineer or Developer team.
- Understand how a central, trusted model registry acts as the hand-off between operations teams and application teams.
- Configure OpenShift AI model registry and AI pipelines. 
- In the ModelOps tutorial, deploy and run a GuideLLM and lm-eval harness OpenShift pipeline.
- In the AI Engineer tutorial, deploy and run a GuideLLM and lm-eval harness OpenShift AI Elyra pipeline that is compiled into a Kubeflow pipeline. 
- See how the benchmark results are stored in the OpenShift AI model registry to easily compare results based on default and custom datasets.   

[[pre-reqs]]
== Pre-reqs
- OpenShift is installed
- OpenShift AI is installed
- OpenShift AI is configured with a GPU
- This tutorial was test with OpenShift AI 2.25 and OpenShift 4.19

== Tutorials
* <<docs/modelops_tutorial.adoc#model-ops, ModelOps Tutorial>> (est. 1 hour)
** For the ModelOps or IT Operations role that covers part 1 of the solution presented above.
* <<docs/ai_engineer_tutorial.adoc#ai-engineer, AI Engineer Tutorial>> (est. 30 minutes)
** For the AI Engineer or Application developer role that covers part 2 of the solution presented above.
+
NOTE: The ModelOps tutorial must be completed before you can start the AI Engineer tutorial.